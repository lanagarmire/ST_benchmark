#!/bin/bash
#SBATCH --job-name=real_data_getMOFA+_factor        ## Name of the job for the scheduler
#SBATCH --account=lgarmire             ## name of the resource account. Does not change
#SBATCH --partition=batch              ## queue to submit the job to. Does not change
#SBATCH --nodes=1                      ## number of nodes you are requesting
#SBATCH --ntasks=1                     ## how many resource spaces do you want to reserve
#SBATCH --cpus-per-task=2              ## number of cores/threads needed
#SBATCH --nodelist=garmire-gpu01       ## If you need a GPU, request the GPU node
#SBATCH --mem=10G                     ## Memory requested per job
#SBATCH --time=20:00:00                ## Amount of time you are reserving for this job
#SBATCH --mail-user=liyijun@umich.edu  ## send email notifications to umich email listed
#SBATCH --mail-type=ALL                ## when to send email (standard values are:
#SBATCH --array=15-19                                       ## NONE, BEGIN, END, FAIL, REQUEUE, ALL.  See
#SBATCH --output=outfiles/%x-%A-%a               ## output and error info written to the file listed

if [[ $SLURM_JOB_NODELIST ]] ; then
   echo "Running on"
   scontrol show hostnames $SLURM_JOB_NODELIST
fi

data_path=data_analysis
data_ref=real_data/Visium
diff_in_cov=0.15
svg_pval_thres=0.000005
ndims=30
scVI_error=reconstruction
SNF_metric=sqeuclidean
WNN_smooth=0
CIMLR_k=10
save_name=ndims
n_random_reps=5

# With Slurm, you can load your modules in the SBATCH script
module load lib/python3/cupy/8.0.0+cu101

python real_data_getMOFA+.py $SLURM_ARRAY_TASK_ID $data_path $data_ref $diff_in_cov $svg_pval_thres $ndims $scVI_error $SNF_metric $WNN_smooth $CIMLR_k $save_name $n_random_reps
