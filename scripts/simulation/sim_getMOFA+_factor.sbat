#!/bin/bash
#SBATCH --job-name=sim_getMOFA+_factor        ## Name of the job for the scheduler
#SBATCH --account=lgarmire             ## name of the resource account. Does not change
#SBATCH --partition=batch              ## queue to submit the job to. Does not change
#SBATCH --nodes=1                      ## number of nodes you are requesting
#SBATCH --ntasks=1                     ## how many resource spaces do you want to reserve
#SBATCH --cpus-per-task=2              ## number of cores/threads needed
#SBATCH --nodelist=garmire-gpu01       ## If you need a GPU, request the GPU node
#SBATCH --mem=1G                     ## Memory requested per job
#SBATCH --time=20:00:00                ## Amount of time you are reserving for this job
#SBATCH --mail-user=liyijun@umich.edu  ## send email notifications to umich email listed
#SBATCH --mail-type=ALL                ## when to send email (standard values are:
#SBATCH --array=1-25                                       ## NONE, BEGIN, END, FAIL, REQUEUE, ALL.  See                     
#SBATCH --output=outfiles/%x-%A-%a               ## output and error info written to the file listed
                  
if [[ $SLURM_JOB_NODELIST ]] ; then
   echo "Running on"
   scontrol show hostnames $SLURM_JOB_NODELIST
fi

data_path=simulation/simulation_07202021
thres=0.6
data_ref=ST_MOB1
diff_in_cov=0.1
svg_pval_thres=0.001
ndims=15
scVI_error=reconstruction
SNF_metric=sqeuclidean
WNN_smooth=0
CIMLR_k=10
save_name=svg_pval_thres
n_random_reps=5

# With Slurm, you can load your modules in the SBATCH script
module load lib/python3/cupy/8.0.0+cu101

python sim_getMOFA+.py $SLURM_ARRAY_TASK_ID $data_path $thres $data_ref $diff_in_cov $svg_pval_thres $ndims $scVI_error $SNF_metric $WNN_smooth $CIMLR_k $save_name $n_random_reps
